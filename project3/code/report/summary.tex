% http://www-lmmb.ncifcrf.gov/~toms/latexforbeginners.html
% ftp://ftp.cfar.umd.edu/pub/louiqa/


\documentclass[11pt,letterpaper,oneside]{article}
\usepackage[top=1.2in,left=1.3in,right=1.3in,bottom=1.2in]{geometry}
\usepackage{fancyvrb}
\usepackage{colortbl}
\usepackage{graphicx}

\title{Summary for Project 3}
\author{Dejun Qian}

\begin{document}
\maketitle

\section{The algorithm}
The algorithm named "sample" is chosen and implemented in this project. We choose this algorithm because this algorithm can balance the work to each work node very well. The original data is produced by a program named "datagen". Without looking into the implementation detail, we don't know the distribution of the data. The only information we know is that all the data are unsigned integers. As the range of the data values is known in advance, bucket sort is a good choice here. This kind of algorithms can use more information than the compare-based algorithms to acheive better performance. In general parallel bucket sort algorithm, the interval is divided into equal-size regions. However, this will cause the total work distributed with large difference. Some of the nodes may get more work to do than others. The algorithm "sample" can use the information of the original data and create buckets adaptively.


\section{Implementation}
The algorithm is implemented with two main parts. The first is the IO part which is responsible for reading and writing files on the filesystem. The second part is the sorting part which handles the sorting job in memory. This part of job is done is parallel.

IO part is implemented in a single node, node 0 is used in this implementation. This part is pretty straight forward. All the nodes begin executing at the same time, but all other nodes begin waiting the initial data from node 0 without doing the IO job. Differently, node 0 begins with file reading, it reads the whole data from the file into memory at one time. After all the data are read in, the node 0 parts the data and send each part to every other node. Same thing happens at the end the program, only node 0 does the writing job which writes back the sorted data into the destinate file located in the filesystem. The procedure looks like this: aftering finishing its own job, node 0 waits for the result from other nodes, and after successfully received all the data, it begins the writing job which writes the result from memory into disk. The IO part in this implementation only resident in the beginning and the final part of the whole program and is all handled by node 0. By using single node to handle the IO part, it's easy for time measuring. We can easily get the information about the performance gain compared to the result from project 2. Actually, this algorithm can fit with multi-IO very well, as all the nodes begin begin with a continious part of data from input file and end with a continious part of data in the destination file. This way, the time will be saved not only from the overlapped IO operations, but also from the elimination of data dispatch and data collection which are needed in single IO implementation.

The sorting part is implemented by adopting the "sample" algorithm. It's composed of the following parts,
%\begin{itemize}
\begin{enumerate}
\item Distribute the unsorted data read from the input file evenly to each node.
\item Each node sorts its data received locally using local quick sort algorithm.
\item Let's say there is N nodes. Then each node divides the sorted data evenly into N buckets and collects the seperating points.
\item Node 0 collets all the seperating points and sorts these data with quick sort algorithm.
\item The global seperating points which are used to form the buckets for the whole data is generated by selecting N-1 even-interval points from the sorted list produced in the last step. Then the global seperating points is send to each node.
\item Each node put its sorted data into corresponding bucket generated from the global seperating points and send partition j to node j.
\item Each node merges all the data pieces it received along with its own part.
\item Each node send the sorted data back to node 0. All pieces are put togather to form the sorted result.
%\end{itemize}
\end{enumerate}
Different from project 1 and project 2, this project is run in a NUMA system. Every data sharing should be done by message passing machenism. Communications in this implementation happens in step 1, step 3, step 5, step 6 and step 8. In step 1 and step 5, one-to-many communication is implemented. In step 3 and step 8, many-to-one communication is implemented. In step 6, many-to-many communication happens.

Step 6 is the most complicate one which need each node to send data to every other node. Without thinking more, I came to the idea that each node begins with doing all the sending job first, then does the receiving jobs. The pseudocode is shown bellow,
\begin{Verbatim}[frame=single]
line 1: // sending data out
line 2: for each node i
line 3:   if i != rank
line 4:     send the packet size to i
line 5:     send the packet data to i
line 6:
line 7: // receiving data in
line 8: for each node i
line 9:   if i != rank
line 10:    receive the packet size from i
line 11:    receive the packet data from i
\end{Verbatim}
Unfortunately, the program refuses executing after finished line 4. When testing, this piece of code will not work even only two nodes are employed when the data size is larger than 2500. With smaller packets, it works just fine. No accurate answer figured out for this problem. One possible answer is that a burst of sending jobs are created and no one will actually be received, and all these data will be cached in the MPI environment which lead to some malfunction result. Maybe it's the problem of the MPI environment itself.

As a workaround, I designed another implementation. In this implementation, instead of sending all the data out first for every node, I carefully rearrange the sending jobs and the receiving jobs in different order according to the node, where each sending job is assured to have its destination ready to receive. This implementation is composed of N steps (N equals to the number of nodes). The new pseudocode is shown bellow,
\begin{Verbatim}[frame=single]
line 1: // sending data out
line 2: for each step i
line 3:   if i == rank
line 4:     send the packet size to every other node
line 5:     send the packet data to every other node
line 6:   else
line 7:     receive the packet size from every other node
line 8:     receive the packet data from every other node
\end{Verbatim}
Now this code works when host number ranges from 1 to 8 with the data size ranges from 500 to 1000000. However, when the host number comes to 16, it will stop arround step 13. I tried my best, but still can't figure out why. The problem of the MPI environment or the problem of my own code?

\section{Timing results}
We evaluate our implementation with 10 input sizes: 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000, 10000000 and 50000000. When testing the code, we set host number to 4 different value: 1, 2, 4 and 8 by creating 4 different host list files. Each data size is combined with every host number size when doing the test. We collected the time for with or without IO scenary. Every combination is executed for 10 times, the shortest time is recorded. The result is shown bellow. The rows marked with "Node\#1 - Node\#8" are the result for each node number. The light gray rows indicate the result for the time without IO while the heavy gray rows indicate the result for the time with IO operation. Each column gives the results for each input size.

Intuitively, large data needs more time than small data, time with IO should be longer than time without IO, more nodes lead to less time. The first two seems right from the table, however, the third one is a little complicated. The experiment result shows that the time used to finish the job is not always shorter when more nodes are introduced. In fact, the table shows that the time goes up as more nodes are used when data size is less than 10000. This is mainly because of the communication overhead. When data size is small, the sorting task itself takes short time to finish. As a result, the percentage of the communication time goes up, and begin to dominate the whole running time. With more nodes, the communication complexity goes up, and takes more time. This comes to the final result that time increases when more nodes are adopted. 

% \input{prime_seq.tex}
\begin{table}[th]
\caption{Result for Sample sort}
%\tiny
\centering
\scalebox{0.825}{
\begin{tabular}{l||cccccccccc}
\hline
 & \multicolumn{10}{c}{Array Size}\\
\cline{2-11}
 & 1000 & 5000 & 10000 & 50000 & 100000 & 500000 & 1000000 & 5000000 & 10000000 & 50000000 \\
\hline\hline
\input{table.tex}
\hline
\end{tabular}
}
\label{table:table1}
\end{table}


%\begin{equation}
%\sqrt{\pi}
%\end{equation}

\section{General comments}
When choosing the parallel algorithms, the communication overhead shouldn't be ignored. One should give priority to the algorithms that can exploit more parallelism and do more work on the local node. From this point of view, the algorithm adopted in this project is not a very good, as it comminication overhead is O($N^{2}$). Based on the experiment result from last section, I would say it's worth to employ more nodes when there are a huge size of data. However, the improvement is not as much as expected, especially when the number of nodes goes high.

\end{document}
